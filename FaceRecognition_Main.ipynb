{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"facerecognition.ipynb","provenance":[{"file_id":"153n_ajpEzddidP6Zjpu9GyQJ_euAecbC","timestamp":1598177743437},{"file_id":"1v4zM9Gcxt6r5pHGN8HS6CYsLTt1VoZsG","timestamp":1598040967481},{"file_id":"1VXSGL_Cpxo2l5vxhxMdrWEtVs-zC0078","timestamp":1575301440584}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"r-gvYsnzmhgz","colab_type":"code","colab":{}},"source":["from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","import numpy as np\n","from PIL import Image\n","import io\n","import cv2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cuVXP5xqxLgs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"ok","timestamp":1598177922087,"user_tz":-180,"elapsed":4832,"user":{"displayName":"Fatima Lakkis","photoUrl":"","userId":"10340656259968186196"}},"outputId":"80b4be25-ecf5-4c29-d9f0-7383866dd554"},"source":["!wget https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-08-23 10:18:39--  https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 930127 (908K) [text/plain]\n","Saving to: ‘haarcascade_frontalface_default.xml’\n","\n","\r          haarcasca   0%[                    ]       0  --.-KB/s               \rhaarcascade_frontal 100%[===================>] 908.33K  --.-KB/s    in 0.05s   \n","\n","2020-08-23 10:18:39 (18.0 MB/s) - ‘haarcascade_frontalface_default.xml’ saved [930127/930127]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N6sZV5rloIdr","colab_type":"code","colab":{}},"source":["def VideoCapture():\n","  js = Javascript('''\n","    async function create(){\n","      div = document.createElement('div');\n","      document.body.appendChild(div);\n","\n","      video = document.createElement('video');\n","      video.setAttribute('playsinline', '');\n","\n","      div.appendChild(video);\n","\n","      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n","      video.srcObject = stream;\n","\n","      await video.play();\n","\n","      canvas =  document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","\n","      div_out = document.createElement('div');\n","      document.body.appendChild(div_out);\n","      img = document.createElement('img');\n","      div_out.appendChild(img);\n","    }\n","\n","    async function capture(){\n","        return await new Promise(function(resolve, reject){\n","            pendingResolve = resolve;\n","            canvas.getContext('2d').drawImage(video, 0, 0);\n","            result = canvas.toDataURL('image/jpeg', 0.8);\n","            pendingResolve(result);\n","        })\n","    }\n","\n","    function showimg(imgb64){\n","        img.src = \"data:image/jpg;base64,\" + imgb64;\n","    }\n","\n","  ''')\n","  display(js)\n","\n","def byte2image(byte):\n","  jpeg = b64decode(byte.split(',')[1])\n","  im = Image.open(io.BytesIO(jpeg))\n","  return np.array(im)\n","\n","def image2byte(image):\n","  image = Image.fromarray(image)\n","  buffer = io.BytesIO()\n","  image.save(buffer, 'jpeg')\n","  buffer.seek(0)\n","  x = b64encode(buffer.read()).decode('utf-8')\n","  return x\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f20cjZvWqr3R","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1598177956058,"user_tz":-180,"elapsed":27691,"user":{"displayName":"Fatima Lakkis","photoUrl":"","userId":"10340656259968186196"}},"outputId":"3a47666e-ac53-45d5-d135-e8ca3807d163"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3PYhbnfcTV1Z","colab_type":"code","colab":{}},"source":["face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n","detector = dlib.get_frontal_face_detector()\n","sp = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n","model = dlib.face_recognition_model_v1('dlib_face_recognition_resnet_model_v1.dat')\n","FACE_DESC, FACE_NAME = pickle.load(open('trainset.pk', 'rb'))\n","\n","VideoCapture()\n","eval_js('create()')\n","while True:\n","    byte = eval_js('capture()')\n","    frame = byte2image(byte)\n","    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n","    faces = face_detector.detectMultiScale(gray, 1.3, 5)\n","    for (x, y, w, h) in faces:\n","        img = frame[y-10:y+h+10, x-10:x+w+10][:,:,::-1]\n","        dets = detector(img, 1)\n","        for k, d in enumerate(dets):\n","            shape = sp(img, d)\n","            face_desc0 = model.compute_face_descriptor(img, shape, 1)\n","            d = []\n","            for face_desc in FACE_DESC:\n","                d.append(np.linalg.norm(np.array(face_desc) - np.array(face_desc0)))\n","            d = np.array(d)\n","            idx = np.argmin(d)\n","            if d[idx] < 0.5:\n","                name = FACE_NAME[idx]\n","                #print(name)\n","                cv2.putText(frame, name, (x, y-5), cv2.FONT_HERSHEY_COMPLEX, .7, (255,255,255),2)\n","                cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n","    eval_js('showimg(\"{}\")'.format(image2byte(frame)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZsiqjGxtPo8f","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from keras.preprocessing import image\n","\n","\n","EMOTIONS_LIST = [\"Angry\", \"Disgust\",\n","                     \"Fear\", \"Happy\",\n","                     \"Sad\", \"Surprise\",\n","                     \"Neutral\"]\n","\n","Age_LIST = [\"Adult\", \"Kid\",\n","                     \"Senior\", \"Teenager\"]\n","\n","Gender_LIST = [\"Male\", \"Female\"]\n","\n","\n","def LoadModel(model_json_file, model_weights_file):\n","   # load model from JSON file\n","    with open(model_json_file, \"r\") as json_file:\n","         loaded_model_json = json_file.read()\n","         loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n","    return loaded_model_json,loaded_model\n","\n","\n","emotion_model_json,emotional_loaded_model=LoadModel(\"/content/drive/My Drive/Bootcamp/face_model.json\",\"/content/drive/My Drive/Bootcamp/face_model.h5\")\n","\n","age_model_json,age_loaded_model=LoadModel(\"/content/drive/My Drive/Bootcamp/modelAge.json\",\"/content/drive/My Drive/Bootcamp/modelAge.h5\")\n","\n","gender_model_json,gender_loaded_model=LoadModel(\"/content/drive/My Drive/Bootcamp/modelGender.json\",\"/content/drive/My Drive/Bootcamp/modelGender.h5\")\n","\n","def predict_emotion(img,loaded_model):\n","    preds = loaded_model.predict(img,batch_size=10)\n","    #print(EMOTIONS_LIST[np.argmax(preds)])\n","    return EMOTIONS_LIST[np.argmax(preds)]\n","\n","\n","def predict_age(img,loaded_model):\n","    preds = loaded_model.predict(img,batch_size=10)\n","    #print(Age_LIST[np.argmax(preds)])\n","    return Age_LIST[np.argmax(preds)]\n","\n","\n","def predict_gender(img,loaded_model):\n","    preds = loaded_model.predict(img,batch_size=10)\n","    print(preds)\n","    print(Gender_LIST[np.argmax(preds)])\n","    return Gender_LIST[np.argmax(preds)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6sZQW277PyVK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":699},"executionInfo":{"status":"error","timestamp":1598178234888,"user_tz":-180,"elapsed":522,"user":{"displayName":"Fatima Lakkis","photoUrl":"","userId":"10340656259968186196"}},"outputId":"765ec240-624e-4544-b743-8ea5863b6cd8"},"source":["face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n","#detector = dlib.get_frontal_face_detector()\n","#sp = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n","#model = dlib.face_recognition_model_v1('dlib_face_recognition_resnet_model_v1.dat')\n","#FACE_DESC, FACE_NAME = pickle.load(open('trainset.pk', 'rb'))\n","\n","VideoCapture()\n","eval_js('create()')\n","while True:\n","    byte = eval_js('capture()')\n","    frame = byte2image(byte)\n","    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n","    faces = face_detector.detectMultiScale(gray, 1.3, 5)\n","    for (x, y, w, h) in faces:\n","        #img = frame[y-10:y+h+10, x-10:x+w+10][:,:,::-1]\n","        fc = gray[y:y+h, x:x+w]\n","\n","        roi = cv2.resize(frame, (150, 150))\n","        k = image.img_to_array(roi)\n","        k = np.expand_dims(k, axis=0)\n","        images = np.vstack([k])\n","        #pred = model.predict_emotion(images)\n","\n","        #predict Gender\n","        gender=predict_gender(images,gender_loaded_model)\n","\n","        #predict Age\n","        age=predict_age(images,age_loaded_model)\n","\n","        roi = cv2.resize(fc, (48, 48))\n","        #pred = cnn.predict_emotion(roi[np.newaxis, :, :, np.newaxis])\n","\n","        # predict Emotion\n","        emotion=predict_emotion(roi[np.newaxis, :, :, np.newaxis],emotional_loaded_model)\n","\n","        Result= gender + \" \" + age + \" \" + emotion\n","\n","\n","        cv2.putText(frame, Result, (x, y-5), cv2.FONT_HERSHEY_COMPLEX, .7, (255,255,255),2)\n","        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n","    eval_js('showimg(\"{}\")'.format(image2byte(frame)))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"MessageError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-964d467ad072>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'create()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mbyte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'capture()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbyte2image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_RGB2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: Cell has no view"]}]}]}